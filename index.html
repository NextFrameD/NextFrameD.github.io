<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NFD</title>
    <link rel="shortcut icon" type="image/jpg" href="" />
    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css"> -->
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>
</head>

<body>
    <section class="section">
        <div class="container is-max-widescreen">
            <h1 class="title is-2 has-text-centered">
                Playing with Transformer at 30+ FPS via Next-Frame Diffusion
            </h1>
            <p class="subtitle is-4 has-text-centered">
                 <br>
            </p>
        </div>
        <div class="container is-max-desktop">
            <p class="subtitle is-6 has-text-centered authors" style="line-height: 1.2;">
                <span>
                    <a href="https://ada-cheng.github.io">Xinle&nbsp;Cheng</a><sup>&#127982;&#127794;</sup>
                </span>
                <span>
                    <a href="https://www.microsoft.com/en-us/research/people/tianyuhe" class="has-tooltip-bottom" data-tooltip="+ indicates project lead">Tianyu&nbsp;He</a><sup>&#127794;+</sup>
                </span>
                <span>
                    <a href="https://www.microsoft.com">Jiayi&nbsp;Xu</a><sup>&#127982;</sup>
                </span>
                <span>
                    <a href="https://leoguojl.me/" >Junliang&nbsp;Guo</a><sup>&#127794;</sup>
                </span>
                <span>
                    <a href="https://dihe-pku.github.io/">Di&nbsp;He</a><sup>&#127982;</sup>
                </span>
                <span>
                    <a href="https://sites.google.com/view/jiangbian">Jiang&nbsp;Bian</a><sup>&#127794;</sup>
                </span>
                
            </p>
            <p class="subtitle is-6 has-text-centered authors" style="line-height: 1.0;">
                <span>
                    <sup>&#127982;</sup>Peking University
                </span>
                <span>
                        <sup>&#127794;</sup>Microsoft
                </span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <a href="http://arxiv.org/pdf/2506.01380" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a>
            <!-- <a href="https://xxx" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fas fa-file"></i></span>
                <span>Supplementary</span>
            </a> -->
            <a href="http://arxiv.org/abs/2506.01380" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <!-- <a href="https://xxx" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fab fa-youtube"></i></span>
                <span>Video</span>
            </a> -->
            <a href="https://microsoft.com" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code (Coming Soon)</span>
            </a>
        </div>
    </section>
    <section class="section pt-0">
<div class="container is-max-desktop">
  <div class="content has-text-centered"> <video controls="" width="70%" autoplay loop muted>
      <source src="video/nfd_v2.mp4" type="video/mp4">
    </video>
    <p>
      <strong>TL;DR:</strong> we present Next-Frame Diffusion (NFD), an autoregressive diffusion transformer that incorporates block-wise causal attention, enabling efficient inference via few-step sampling and parallel token generation. 
    </p>
  </div>
</div>
    </section>
    <section>
        <div class="container">
        <div class="row">
            <div class="col-12 text-center">
                <br>
                <hr style="margin-top:0px">

                <table style="table-layout: fixed; margin: 0 auto;">
                    <caption style="caption-side: top; text-align: center; font-weight: bold; font-size: 18px; padding: 10px;">
                        Videos generated by NFD+ 310M at 31 FPS on an A100 GPU.
                      </caption>
                    <tbody>
                        <tr>
                            <td width="33%"> <video width="100%"  controls autoplay loop muted> <source src="video/clip_200.mp4" type="video/mp4"> </video> </td>
                            <td width="33%"> <video width="100%" controls autoplay loop muted> <source src="video/clip_170.mp4" type="video/mp4"> </video> </td>
                            <td width="33%"> <video width="100%" controls autoplay loop muted> <source src="video/clip_251.mp4" type="video/mp4"> </video> </td>
                        </tr>
                    </tbody>
                    </table>
             
                    <br>
                    <hr style="margin-top:0px">
                    <table style="table-layout: fixed; margin: 0 auto;">
                        <tbody>

                            <tr>
                                <td width="33%"> <video width="100%" controls autoplay loop muted> <source src="video/clip_39.mp4" type="video/mp4"> </video> </td>
                                <td width="33%"> <video width="100%" controls autoplay loop muted> <source src="video/clip_181.mp4" type="video/mp4"> </video> </td>
                                <td width="33%"> <video width="100%" controls autoplay loop muted> <source src="video/clip_232.mp4" type="video/mp4"> </video> </td>
                            </tr>
                        </tbody>
                        </table>
            </div>
        </div>
        </div>
    <br>
    </section>

    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    Autoregressive video models offer distinct advantages over bidirectional diffusion
                    models in creating interactive video content and supporting streaming applications
                    with arbitrary duration. In this work, we present Next-Frame Diffusion (NFD), an
                    autoregressive diffusion transformer that incorporates block-wise causal attention,
                    enabling iterative sampling and efficient inference via parallel token generation
                    within each frame. Nonetheless, achieving real-time video generation remains a
                    significant challenge for such models, primarily due to the high computational
                    cost associated with diffusion sampling and the hardware inefficiencies inherent
                    to autoregressive generation. 
                </p>
                
                <p>
                    To address this, we introduce two innovations: 
                    (1) We extend consistency distillation to the video domain and adapt it specifically for
                    video models, enabling efficient inference with few sampling steps; 
                    (2) To fully leverage parallel computation, motivated by the observation that adjacent frames
                    often share the identical action input, we propose speculative sampling. In this
                    approach, the model generates next few frames using current action input, and
                    discard speculatively generated frames if the input action differs. Experiments
                    on a large-scale action-conditioned video generation benchmark demonstrate that
                    NFD beats autoregressive baselines in terms of both visual quality and sampling
                    efficiency. We, for the first time, achieves autoregressive video generation at over
                    30 Frames Per Second (FPS) on an A100 GPU using a 310M model.
                </p>

            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Architecture
            </h1>
            <div class="content has-text-justified-desktop">
                <img class="mb-5" src="img/arch.png">
                <p>
                    The architecture of NFD contains a tokenizer that transforms raw visual signals to latent representations, and a Diffusion Transformer (DiT) that generates these latents.
                </p>
                <p>
                    We propose a Block-wise Causal Attention mechanism that combines bidirectional attention within each frame and causal dependencies across frames to model spatio-temporal dependencies efficiently.
                    In contrast to the computationally intensive 3D full attention, our approach reduces the overall cost by 50%, enabling hardware-efficient and streaming prediction of all tokens in the next frame in parallel.          
                </p>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Training and Sampling
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    Given a video frame x<sub>i</sub> , we assign an independent timestep t and generate a noised version via linear interpolation: 
                </p>
                <img class="mb-5" src="img/eq1.png">
                <p>
                    Training minimizes the following Flow Matching loss:   
                </p>
                <img class="mb-5" src="img/eq2.png">
                <p>
                    For sampling, we adopt DPM-Solver++, where we recover the denoised frame with:
                </p>
                <img class="mb-5" src="img/eq2.5.png">
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Accelerated Sampling
            </h1>
            <p>
                We introduce a set of methodological advancements aimed at improving the sampling efficiency of NFD, while preserving high visual fidelity in the generated video content.
            </p>
            <div class="content has-text-justified-desktop">
                <p>
                    <h1 class="title is-5">
                        Consistency Distillation
                    </h1>
                    We extend sCM to the video domain, and adapt it to the specific features of video data, where the training objective of the sCM becomes:
                </p>
                <img class="mb-5" src="img/eq4.png">
                <p>
                    <h1 class="title is-5">
                        Speculative Sampling
                    </h1>
                    We introduce a speculative sampling technique designed to accelerate inference by enabling parallel prediction of multiple future frames.
                    After this speculative generation, we compare the predicted actions with the actual subsequent action inputs in the sequence. Once a discrepancy between the predicted and true actions is detected, all subsequent speculative frames beyond that point are discarded, and generation resumes from the last verified frame.
                </p>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Main Results
            </h1>
            <p>
                We present a comparative analysis of our proposed method against state-of-the-art baselines in the following table, highlighting both sampling efficiency and visual quality of the generated videos. 
            </p>
            <img class="mb-5" src="img/tab1.png">
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Visualization: Details Aligned with Physical Properties.<a name="fullcontrol"></a>
            </h1>
            <div class="content has-text-justified-desktop">
                <video controls="" width="100%" autoplay loop muted>
                    <source src="video/fixed_clip_199.mp4" type="video/mp4">
                </video>
                <p>
                    Videos generated by NFD+ and MineWorld respectively, which illustrates a door-opening sequence. NFD+ accurately captures the door's geometry, maintaining
                    its shape and structural integrity. In contrast, MineWorld introduces an artificial line between the
                    two doors and fails to retain detail in the right portion of the door.
                </p>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Visualization: Consistency Across Frames.<a name="fullcontrol"></a>
            </h1>
            <div class="content has-text-justified-desktop">
                <video controls="" width="100%" autoplay loop muted>
                    <source src="video/fixed_clip_90.mp4" type="video/mp4">
                </video>
                <p>
                    Videos generated by NFD+ and MineWorld respectively, which illustrates the superior
                    temporal consistency achieved by NFD+. Despite a significant camera movement, NFD+ preserves a stable and coherent ground, whereas MineWorld introduces visible artifacts and distortions.
                </p>
            </div>
        </div>
    </section>

</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>
